#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage[a4paper]{geometry}
\usepackage{fancyhdr}

\pagestyle{fancy}

\fancyhead[LO,LE]{Peter, Arni}
\fancyhead[CO,CE]{StatML Assignment 1 - Foundations}
\fancyhead[RO,RE]{18 / 02 - 2014}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 4
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language swedish
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
StatML Exam
\begin_inset Newline newline
\end_inset

In a Galaxy Far, Far Away
\end_layout

\begin_layout Author
Arni Asgeirsson lwf986
\end_layout

\begin_layout Date
03/04-2014
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section*
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
- Very short, what is linear regression
\end_layout

\begin_layout Plain Layout
- Just-ish as short, what is our problem
\end_layout

\begin_layout Plain Layout
- How can machine learning help us to fix our problem? <- just very short?
 -> One general sentence, followed by a more specific sentence that mentions
 how linear regression can help us
\end_layout

\begin_layout Plain Layout
- How have I done it? the code
\end_layout

\begin_layout Plain Layout
- What does it do? / What is happening? Theoretically
\end_layout

\begin_layout Plain Layout
- What are my results? (Deliverables)
\end_layout

\begin_layout Plain Layout
- What other methods could I have used?
\end_layout

\begin_layout Plain Layout
- - What is good and bad about my method?
\end_layout

\begin_layout Plain Layout
- - What else could I have used, what are their strength and weaknesses?
\end_layout

\begin_layout Plain Layout
- - Why did I not use those methods, but instead chose to do, what I did?
\end_layout

\end_inset

Introduction
\end_layout

\begin_layout Standard
- What is this?
\end_layout

\begin_layout Standard
- What are my main focuses in this report?
\end_layout

\begin_layout Standard
- What is the structure of my hand in?
\end_layout

\begin_layout Standard
- What is the structure of this report?
\end_layout

\begin_layout Standard
- My general assumptions
\end_layout

\begin_layout Standard
* That the reader is familiar with basic machine learning concepts (and
 statistical stuff) and that the reader is familiar with the given assignment
 text and the given datasets.
\end_layout

\begin_layout Standard
- Have I deviated from the assignment text? If yes, how and why?
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
- The math notation I use ..
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
I focus on discussion my results rather on the deep theory behind my methods,
 as 10 pages are limited.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
As I have used the 
\emph on
sci-kit learn
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
http://scikit-learn.org/stable/
\end_layout

\end_inset

 library extensively through our my code for my machine learning needs,
 the produced code is rather short and somewhat uninterestingly, and will
 therefore not dwell into my specific implementations in this report, but
 rather focus the limited number of pages to briefly discuss the theory
 behind the used methods and mainly focus on a discussion on my results.
\end_layout

\begin_layout Standard
I the reader is interested in the details of my implementations I refer
 to the code files, I have tried to comment my code to the degree that someone
 with minor python skills, and a decent understanding of machine learning
 and linear algebra can understand the code.
\end_layout

\begin_layout Section
Predicting the Specific Star Formation Rate
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
- Short intro to this chapter
\end_layout

\begin_layout Plain Layout
- Any sub-general assumptions
\end_layout

\begin_layout Plain Layout
- What code files are being used here?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the following to exercises we will look at linear and non-linear regression
 for predicting the specific star formation rate (sSFR) for a given galaxy.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
I have used the 
\emph on
linear_model
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegress
ion.html
\end_layout

\end_inset

 and the 
\emph on
????
\emph default
 modules from 
\emph on
sci-kit learn
\emph default
 to fit my models.
\end_layout

\end_inset

 The file 
\emph on
src/question1.py
\emph default
 shows my own implementation of linear regression.
 Although even though it is not 
\emph on
sci-kit learn
\emph default
 the implementation is still rather simple and I will not give a code explanatio
n here.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
If the reader were to look through my code, the reader will notice that
 I actually do use 
\emph on
sci-kit
\emph default
 
\emph on
learn
\emph default
 to perform linear regression along side my own implementation.
 Although this is just to double check that my implementation is correct.
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Remove the footnote?
\end_layout

\end_inset

 In question 2 I have used the 
\emph on
RandomForestRegressor
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegre
ssor.html
\end_layout

\end_inset

 from 
\emph on
sci-kit learn
\emph default
.
\end_layout

\begin_layout Subsection
Question 1 (linear regression)
\begin_inset CommandInset label
LatexCommand label
name "sub:q1_linear_reg"

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
http://glowingpython.blogspot.dk/2012/03/linear-regression-with-numpy.html
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
If we were to be given a set of data points, each describing the 'colours'
 of a galaxy we could use linear regression to create a model that predicts
 the sSFR of the given galaxy.
 Which is exactly what we want to do
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
say something more?
\end_layout

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Already said in the intro?
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Slide: 5_Regression1.pdf page 14
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula 
\[
y(\bar{x},\bar{w})=w_{0}+\sum_{j=1}^{M-1}w_{j}\phi_{j}(\bar{x})
\]

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
y(\bar{x},\bar{w})=\sum_{j=0}^{M}w_{j}\phi_{j}(\bar{x})
\]

\end_inset


\end_layout

\begin_layout Plain Layout
Where 
\begin_inset Formula $M$
\end_inset

 is ?
\begin_inset Note Note
status open

\begin_layout Plain Layout
dimensions?
\end_layout

\end_inset

, 
\begin_inset Formula $\bar{x}=\begin{pmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{N}
\end{pmatrix}$
\end_inset

 is the input, 
\begin_inset Formula $\bar{w}=\begin{pmatrix}w_{0}\\
w_{1}\\
\vdots\\
w_{M-1}
\end{pmatrix}$
\end_inset

 is the weight vector and 
\begin_inset Formula $\bar{\phi}=\begin{pmatrix}\phi_{0}\\
\phi_{1}\\
\vdots\\
\phi_{M-1}
\end{pmatrix}$
\end_inset

 denotes a set of basic functions where 
\begin_inset Formula $\phi_{0}(x)=1$
\end_inset

.
 We can then write
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
y(\bar{x},\bar{w})=\bar{w}^{T}\bar{\phi}(\bar{x})
\]

\end_inset


\end_layout

\begin_layout Plain Layout
What we now need to do is to choose 
\begin_inset Formula $\bar{\phi}$
\end_inset

 and 
\begin_inset Formula $\bar{w}$
\end_inset

.
 As we are using a linear regression model 
\begin_inset Formula $\bar{\phi}(\bar{x})$
\end_inset

 is simply 
\begin_inset Formula $\bar{x}$
\end_inset

, therefore what is left is to determine 
\begin_inset Formula $\bar{w}$
\end_inset

.
\end_layout

\begin_layout Plain Layout
What we want to do is to minimize the expression 
\begin_inset Formula $\sum_{n=1}^{N}(y(\bar{x}_{n},\bar{w})-t_{n})^{2}$
\end_inset

 when 
\begin_inset Formula $y(\bar{x},\bar{w})=\bar{w}^{T}\bar{\phi}(\bar{x})$
\end_inset

 which results in the equation 
\begin_inset Formula $\bar{w}=(\Phi^{T}\Phi)^{-1}\Phi^{T}\bar{t}$
\end_inset

, where
\begin_inset Formula 
\[
\Phi=\begin{pmatrix}\phi_{0}(x_{1}) & \phi_{1}(x_{1}) & \ldots & \phi_{M-1}(x_{1})\\
\phi_{0}(x_{2}) & \phi_{1}(x_{2}) & \ldots & \phi_{M-1}(x_{2})\\
\vdots &  &  & \vdots\\
\phi_{0}(x_{N}) & \phi_{1}(x_{N}) & \ldots & \phi_{M-1}(x_{N})
\end{pmatrix}
\]

\end_inset


\end_layout

\begin_layout Plain Layout
That means that if we have a basic function 
\begin_inset Formula $\phi(x)$
\end_inset

 we can calculate the weight vector 
\begin_inset Formula $\bar{w}$
\end_inset

.
 As we are trying to create a linear model we use the very simple linear
 basic function 
\begin_inset Formula $\phi(x)=x$
\end_inset

, that means that our 
\begin_inset Formula $\Phi$
\end_inset

 looks like the following:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
\Phi=\begin{pmatrix}1 & x_{1} & \ldots & x_{1}\\
1 & x_{2} & \ldots & x_{2}\\
\vdots &  &  & \vdots\\
1 & x_{N} & \ldots & x_{N}
\end{pmatrix},\,\mathbb{R}^{M\times N}
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Correct matrix dimensions?
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
For about 4995 good reasons I will not manually write this matrix nor will
 I otherwise represent it visually in this report, although what I will
 do, and have done, is write a function 
\emph on
w(X,T)
\emph default
 that computes the weight vector, the function is very simple as it is just
 an implementation of the function 
\begin_inset Formula $\bar{w}=(\Phi^{T}\Phi)^{-1}\Phi^{T}\bar{t}$
\end_inset

.
\end_layout

\begin_layout Plain Layout
When the weight vector is found we can insert the values in our linear regressio
n model
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
y(\bar{x},\bar{w})=w_{0}+w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}+w_{4}x_{4}
\]

\end_inset


\end_layout

\begin_layout Plain Layout
As I am using 
\emph on
sci-kit learn
\emph default
 and their 
\emph on
linear_model
\emph default
 for this exercise I could also use custom input vectors to extract the
 weight vector.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Something more about this...
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
As I have used the 
\emph on
linear_model
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegress
ion.html
\end_layout

\end_inset

 module from the 
\emph on
sci-kit learn
\emph default
 library to create and train the linear model the produced code is very
 short and I will therefore not dwell into this, but rather try and dig
 deeper into what is actually happening and discuss my results.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Make it a general assumption, as this is listed in every chapter ? to shorten
 the length of the report
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Linear regression is when we want to find the weight vector 
\begin_inset Formula $\bar{w}=(w_{0},w_{1}...,w_{M})$
\end_inset

 that minimizes 
\begin_inset Formula $\sum_{n=1}^{N}(y(x_{n},\bar{w})-t_{n})^{2}$
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Loose that last bit?
\begin_inset Formula $=\sum_{n=1}^{N}(w_{0}+w_{1}x_{n}-t_{n})^{2}$
\end_inset


\end_layout

\end_inset

,
\begin_inset Note Note
status open

\begin_layout Plain Layout
Is this dimensional general?
\end_layout

\end_inset

 i.e.
 we want find the line that minimizes the sum of errors, which are the squared
 distances from our predicted values to the expected values.
\end_layout

\begin_layout Standard
When performing linear regression we rewrite 
\begin_inset Formula $y$
\end_inset

 to be 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $y(\bar{x},\bar{w})=\sum_{j=0}^{M}w_{j}\phi_{j}(\bar{x})$
\end_inset

, where 
\begin_inset Formula $\phi$
\end_inset

 is some basis function.
 As we are doing linear regression we define 
\begin_inset Formula $\phi_{0}(x)=1$
\end_inset

 and 
\begin_inset Formula $\phi_{i}(x)=x$
\end_inset

, where 
\begin_inset Formula $i\text{≠}0$
\end_inset

.
\end_layout

\begin_layout Standard
As our data is four dimensional our final regression model will be of the
 form:
\begin_inset Formula 
\begin{eqnarray*}
y(\bar{x},\bar{w}) & = & w_{0}+w_{1}\phi_{1}\left(x_{1}\right)+w_{2}\phi_{2}\left(x_{2}\right)+w_{3}\phi_{3}\left(x_{3}\right)+w_{4}\phi_{4}\left(x_{4}\right)\\
y(\bar{x},\bar{w}) & = & w_{0}+w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}+w_{4}x_{4}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
To build or linear regression model we must first define our basis function
 
\begin_inset Formula $\text{\phi}$
\end_inset

 and here we define that to be 
\begin_inset Formula $\phi_{0}(x)=1$
\end_inset

 and for any other 
\begin_inset Formula $\phi(x)=x$
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
write more correctly?
\end_layout

\end_inset

.
 
\end_layout

\end_inset

Now we can move on to find our weight vector 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula $\bar{w}=\begin{pmatrix}w_{0}\\
w_{1}\\
w_{2}\\
w_{3}\\
w_{4}
\end{pmatrix}$
\end_inset


\end_layout

\end_inset


\begin_inset Formula $\bar{w}=\left(w_{0}\, w_{1}\, w_{2}\, w_{3}\, w_{4}\right)^{T}$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Can I write it like this?
\end_layout

\end_inset

.
 We do that by using the derived formula 
\begin_inset Formula $\bar{w}=(\Phi^{T}\Phi)^{-1}\Phi^{T}\bar{t}$
\end_inset

, which is rather simple as when using the linear basis functions, 
\begin_inset Formula $\Phi$
\end_inset

 becomes very simple.
\end_layout

\begin_layout Standard
When doing the calculations I get that the weight vector is 
\begin_inset Formula $\bar{w}=\begin{pmatrix}-8.149433493650\\
-0.794001531121\\
-1.222959203710\\
-0.328584747643\\
-0.786330558664
\end{pmatrix}$
\end_inset

, this gives us the final linear regression model
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y(\bar{x})=-8.14943349365-0.794001531121x_{1}-1.22295920371x_{2}-0.328584747643x_{3}-0.786330558664x_{4}
\]

\end_inset


\end_layout

\begin_layout Standard
Which we now can use to predict the specific star formation rate of a new
 unknown galaxy image.
\end_layout

\begin_layout Standard
This model has a mean square error of 
\begin_inset Formula $0.274754600685$
\end_inset

 on the training data set and a mean square error of 
\begin_inset Formula $0.275179630629$
\end_inset

 on the test data set.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Say something on the mean square error results? -> they are very close to
 each other?
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
\begin_inset Note Note
status collapsed

\begin_layout Subsubsection*
- What other methods could I have used?
\end_layout

\begin_layout Subsubsection*
- - What is good and bad about my method?
\end_layout

\begin_layout Subsubsection*
- - What else could I have used, what are their strength and weaknesses?
\end_layout

\begin_layout Subsubsection*
- - Why did I not use those methods, but instead chose to do, what I did?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Question 2 (non-linear regression)
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
We are still trying to solve the same problem, although now we try out 
\emph on
non
\emph default
-linear regression.
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Subsubsection*
- How have I done it? the code
\end_layout

\begin_layout Plain Layout
I decided to use random forests to perform the nonlinear regression 
\begin_inset Note Note
status open

\begin_layout Plain Layout
enough, now talk a little about how it works..
 some formulas and stuff
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
- What does it do? / What is happening? Theoretically
\end_layout

\end_inset


\end_layout

\begin_layout Standard
When one is imagining a linear line in some space and then 5000 data points,
 one quickly starts to wonder what the chances are that the data really
 is best fit with a linear model.
 This is where non-linear regression comes in, we suspect that our data
 is not linear and therefore try to fit a non-linear regression model to
 our data
\begin_inset Note Note
status open

\begin_layout Plain Layout
sounds like a said the same thing 5 times
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection*
Results
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
* How did I achieve good generalization performance?
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
When performing the regression
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
on the training and test data
\end_layout

\end_inset

 I get a mean-squared error of 
\begin_inset Formula $??\%$
\end_inset

 on the training data and a mean-squared error of 
\begin_inset Formula $??\%$
\end_inset

 on the test data.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
The fact that both the training error and test error are that much less
 than what we saw in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:q1_linear_reg"

\end_inset

 when doing linear regression could hint at two things.
 Either my model has overfitted the data or a non-linear regression model
 is a more suitable choice for the sSFR data (or a mix of the two).
 Considering that both the training error and the test error are greatly
 reduced and the training error is not very close to 
\begin_inset Formula $0$
\end_inset

 are venture myself out on a limb and conclude that my non-linear regression
 model has only overfitted the data a little, if at all, and instead conclude
 that a non-linear regression model fits the sSFR data better.
\begin_inset Note Note
status collapsed

\begin_layout Subsubsection*
- What other methods could I have used?
\end_layout

\begin_layout Plain Layout
Could have used support vector machines or neural networks ..
\end_layout

\begin_layout Subsubsection*
- - What is good and bad about my method?
\end_layout

\begin_layout Subsubsection*
- - What else could I have used, what are their strength and weaknesses?
\end_layout

\begin_layout Subsubsection*
- - Why did I not use those methods, but instead chose to do, what I did?
\end_layout

\end_inset


\end_layout

\begin_layout Section
Stars vs.
 Galaxies
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
- Short intro to this chapter
\end_layout

\begin_layout Plain Layout
- Any sub-general assumptions
\end_layout

\begin_layout Plain Layout
- What code files are being used here?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now we move on to classification where we want to assign a label 
\emph on
l
\emph default
 from a finite set of labels 
\emph on
L
\emph default
 to each data point in the function range, as opposed to regression which
 we did in the previous assignment.
 We are given a train and test data set both containing 
\begin_inset Formula $3000$
\end_inset

 points either belonging to the star class 
\begin_inset Formula $0$
\end_inset

 or the galaxy class 
\begin_inset Formula $1$
\end_inset

.
 Initially we will look at binary classification of the data following by
 principal component analysis and clustering.
\end_layout

\begin_layout Standard
In the following three exercises/questions I have used the 
\emph on
svm
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/svm.html
\end_layout

\end_inset

, 
\emph on
PCA
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
\end_layout

\end_inset

 and 
\emph on
KMeans
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
\end_layout

\end_inset

 modules from 
\emph on
sci-kit learn
\emph default
 for my machine learning needs.
\end_layout

\begin_layout Subsection
Question 3 (binary classification using support vector machines)
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
We are now interested in classifying an given object (described in a set
 of features) to be either a star or a galaxy.
 We denote a galaxy to have the label 
\begin_inset Formula $0$
\end_inset

 and a star to have the label 
\begin_inset Formula $1$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Kinda already said in the intro?
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Why do we use machine learning for such a task? Well we are able to extract
 a lot of information from the different kind of images taken of the space
 objects, al though when they get further and further away, it becomes very
 difficult to distinguish between the objects when looking at simple features.
 Instead we can use machine learning, and especially classification, to
 try and find patterns in the data and a suitable space where the data is
 linear separable
\begin_inset Note Note
status open

\begin_layout Plain Layout
the last part is not entirely correct...
\end_layout

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Seems kinda silly to keep?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
I have used the support vector machines (SVM) to try and perform the binary
 classification of the stars & galaxy data.
 The specified kernels to be used are the radial Gaussian kernels of the
 form:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(\bar{x},\bar{z})=e^{(-\gamma||\bar{x}-\bar{z}||^{2})}
\]

\end_inset


\end_layout

\begin_layout Standard
As I have used the 
\emph on
svm
\emph default
 module from the 
\emph on
sci-kit learn
\emph default
 library, this kernel is the default one and goes by the keyword 
\emph on
'rbf'
\emph default
.
\end_layout

\begin_layout Subsubsection*
\begin_inset Note Note
status collapsed

\begin_layout Subsubsection*
- What does it do? / What is happening? Theoretically
\end_layout

\begin_layout Subsubsection*
- - The svm part
\end_layout

\begin_layout Subsubsection*
- - The kernel trick
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Procedure
\end_layout

\begin_layout Standard
There are several hyperparameters one could delve in to although we will
 concentrate on the regularization constant 
\begin_inset Formula $C$
\end_inset

 and the hyperparameter 
\begin_inset Formula $\gamma$
\end_inset

 for our kernel
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
what are those values?
\end_layout

\end_inset

.
 It can be very hard to find the best hyperparameters to a given model,
 especially when the range of possible good hyperparameters and the number
 of parameters increases, as it simply gets very computationally expensive,
 and SVMs are already very computationally expensive.
\end_layout

\begin_layout Standard
To train my classifier I have used grid search and 
\begin_inset Formula $5$
\end_inset

-fold cross validation to find the best values for 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

, and I have used Jaakkola's heuristic to determine the initial value for
 
\begin_inset Formula $\gamma$
\end_inset

.
 Then I train the final C-SVM classifier with the found values for 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 and use the final classifier to predict the labels for the train and test
 data.
 I must also mention that the hole procedure is done on normalized data
\begin_inset Note Note
status open

\begin_layout Plain Layout
why normalize?
\end_layout

\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
Collapse?
\end_layout

\end_inset

I will proceed to determine some initial values for 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 then use grid search and cross validation to try and determine good values
 for the two hyperparameters.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
As the assignment text so lightly hints at I will use 
\emph on
Jaakkola
\emph default
's heuristic to determine the initial value of 
\begin_inset Formula $\gamma$
\end_inset

.
 
\end_layout

\end_inset

As the assignment text is very clear on how to compute the 
\begin_inset Formula $\sigma_{Jaakkola}$
\end_inset

 and 
\begin_inset Formula $\gamma_{Jaakkola}$
\end_inset

 values, I will refer to my code 
\emph on
src/module/common.py
\emph default
 if the reader is interested in how I coded the heuristic.
 With no further due my computed values are:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{array}{c}
\sigma_{Jaakkola}=1.81188376031\\
\gamma_{Jaakkola}=0.15230330910
\end{array}
\]

\end_inset


\end_layout

\begin_layout Standard
My initial 
\begin_inset Formula $C$
\end_inset

 value is 
\begin_inset Formula $10$
\end_inset

 and using the given combination sets gives me the possible hyperparameters:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Cs:=\{0.01,\,0.1,\,1,\,10,\,100,\,1000\}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Gs:=\{0.0001523033091,\,0.001523033091,\,0.01523033091,\,0.1523033091,\,1.523033091,\,15.23033091,\,152.3033091\}
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Results
\end_layout

\begin_layout Standard
Now after using the above initial 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 values in a grid search and 5-fold cross validation I find that the optimal
 combination of hyperparameters from the given sets above are 
\begin_inset Formula $C=1000$
\end_inset

 and 
\begin_inset Formula $\gamma=0.01523033091$
\end_inset

.
\end_layout

\begin_layout Standard
Now by using these newly found values I train my classifier 
\emph on
clf
\emph default
 with the optimal hyperparameters and is ready to try it out.
\end_layout

\begin_layout Standard
Using 
\emph on
clf
\emph default
 to classify the test data set I get an accuracy of 
\begin_inset Formula $99.9333\%$
\end_inset

 on the train data and an accuracy of 
\begin_inset Formula $99.6\%$
\end_inset

 on the test data.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
I have tried to play around with the hyperparameters, but have yet to produce
 a combination that yielded a massive increase in the test accuracy
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Come with an example
\end_layout

\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The very high accuracy of the training data does bother and concern me a
 little bit, as it unfortunately is a very good indication of overfitting,
 but then again the accuracy of the test data is also very very high, and
 somehow soothes my nerves, but not quite enough.
 If the test data is very close in space to the training data, then the
 accuracy of the test data is not as soothing and my classifier 
\emph on
clf
\emph default
 might be a victim of overfitting, but it is hard to tell without any more
 data.
 It might be that a more optimal combination of hyperparameters would be
 a set that increases the accuracy of the test data, and possible lowers
 the accuracy of the training data.
 
\end_layout

\begin_layout Standard
The mean square error of the training data is 
\begin_inset Formula $0.00066667$
\end_inset

 and the mean square error of the test data is 
\begin_inset Formula $0.004$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
useful to know?
\end_layout

\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Plain Layout
As we can see that even though this method provides some good results it
 is very computational expensive, as we use grid search to try and find
 the most optimal hyperparameters, and there might be a concern of overfitting
\begin_inset Note Note
status open

\begin_layout Plain Layout
Then what? Why am I saying this? -> is a good choice/classifier to use?
\end_layout

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
As we can see SVMs for binary classification can be pretty good, but it
 comes at the cost of computation time and expensive hyperparameter selection,
 some of the other methods that could have been used for the is the linear
 discriminant analysis (LDA) technique or even the perceptron method, although
 that would require that the data is linear separable which I highly doubt
 they are in our case, but then we could use kernels with our perceptron
 to project the data into a space where they indeed are linear separable.
 I have a hard time expecting those two methods to perform better then the
 trained SVM (unless the SVM is way overfitted).
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
I could have used LDA, perceptron, linear classification ..
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Subsubsection*
- - What is good and bad about my method?
\end_layout

\begin_layout Plain Layout
Difficult and possible very computational expensive to find good hyperparameters
, overfitting?, although provides very nice results.
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Subsubsection*
- - What else could I have used, what are their strength and weaknesses?
\end_layout

\begin_layout Subsubsection*
- - Why did I not use those methods, but instead chose to do, what I did?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Question 4 (principal component analysis)
\begin_inset Note Note
status collapsed

\begin_layout Subsubsection*
- Very short, what is pca?
\end_layout

\begin_layout Plain Layout
- see below
\end_layout

\begin_layout Subsubsection*
- Just-ish as short, what is our problem
\end_layout

\begin_layout Plain Layout
- see below
\end_layout

\begin_layout Subsubsection*
- How can machine learning help us to fix our problem? <- just very short?
 -> One general sentence, followed by a more specific sentence that mentions
 how linear regression can help us
\end_layout

\begin_layout Plain Layout
- see below
\end_layout

\begin_layout Subsubsection*
- - To sum up the three above - why are we doing pca and these plots? Why
 are they interesting?
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Have to mention why I normalize? http://stats.stackexchange.com/questions/69157/wh
y-do-we-need-to-normalize-data-before-analysis
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now we will try to use principal component analysis (PCA) on the galaxy
 training data from the 
\emph on
SGTrain2014.dt
\emph default
 training set.
\end_layout

\begin_layout Standard
One way to describe PCA is to say that it is a technique to try and find
 the meaning of the madness by sorting the variance in the data by size.
 A little more technically description is to say that 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
PCA transforms the data in such a way that the greatest variance lie on
 the first axis and the second greatest on the second axis and so on.
\begin_inset Note Note
status open

\begin_layout Plain Layout
The above line and the one below could be combined into one
\end_layout

\end_inset


\end_layout

\end_inset

PCA uses the eigenvalues and eigenvectors of the data to transform
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
 (rotate and move)
\end_layout

\end_inset

 the data so that the greatest eigenvector is aligned with the first axis
 of the coordinate system, and that the 
\emph on
n
\emph default
'th greatest eigenvector is aligned with the 
\emph on
n
\emph default
'th axis, hence the PCA can project the data in dimensions equal to or lower
 than the input dimension.
 This also means that PCA can only be performed once one a given data set,
 as doing it multiply times will not result in any new changes as the data
 is already 'perfectly' aligned with the axis
\begin_inset Note Note
status open

\begin_layout Plain Layout
Is it good enough to keep?
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Why would one want to use PCA? As hinted above PCA can be a very good tool
 to extract the most important features of some dataset, to get rid of some
 noise parameters or visualize some high dimensional data, in a humanly
 perceptually way.
 Which is exactly what we want to do in this exercise for the given galaxy
 data.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Plain Layout
As already stated I have used the 
\emph on
PCA
\emph default
 module from the 
\emph on
sci-kit learn
\emph default
 library to perform the actual principle component analysis and can be viewed
 in the 
\emph on
src/question4.py
\emph default
 file
\begin_inset Note Note
status open

\begin_layout Plain Layout
so what? delete this line?
\end_layout

\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Once again the code written to perform the principal component analysis
 is incredible simple and I will therefore go directly to a presentation
 and discussion of my results.
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Note that I am well aware that 
\emph on
sci-kit learn
\emph default
 uses singular value decomposition to do the PC analysis which is not something
 we have covered in the lectures, although I have decided to abstract from
 this fact and consider PCA in a more general fashion.
\begin_inset Note Note
status open

\begin_layout Plain Layout
I am sure we haven't covered this?
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Results
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2_4_1_combined"

\end_inset

 shows a plot of the eigenspectrum (the green and squared line) which describes
 the size of the eigenvalues.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Figures/2_4_1-combined_eigen.png
	lyxscale 60
	scale 70

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:2_4_1_combined"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2_4_1_combined"

\end_inset

 also shows a plot of how much the eigenvalues/eigenvectors cumulatively
 cover the variance
\begin_inset Note Note
status open

\begin_layout Plain Layout
rephrase, even correct?
\end_layout

\end_inset

 (the purple and circled line).
 We can see that by only looking at the first principle component we have
 already covered about 
\begin_inset Formula $85\%$
\end_inset

 the data.
 This can be useful to determine how many principal components one should
 look at when doing dimensionality reduction, as based on the figure one
 might argue that the added computation time is not worth the last 
\begin_inset Formula $2\%$
\end_inset

, when going from 
\begin_inset Formula $4$
\end_inset

 to 
\begin_inset Formula $10$
\end_inset

 principle components, or if one like to live dangerously one could deem
 those last percentage to be noise.
 We can see how a diminishing return effect occurs when increasing the number
 of principal components to the increase (or the loss of decrease) in data
 size and computation time.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2_4_3_scatterpoints"

\end_inset

 is a great example of using PCA to do dimensionality reduction allowing
 for the ability to visualize a high dimensionality data set.
 The input data is reduced to the first two principal components i.e.
 reduced to 
\begin_inset Formula $2$
\end_inset

 dimensions.
 The plot also shows the eigenvectors (multiplied by the root of their respectiv
e eigenvalue) which shows that they are beautifully 'sorted'
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
 (sorted -> the longest vector is aligned with the first axis and the second
 longest with the second vector)
\end_layout

\end_inset

 and aligned with the two axes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Figures/2_4_2-scatter.png
	lyxscale 60
	scale 70

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:2_4_3_scatterpoints"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Seems not to fit in this exercise
\end_layout

\begin_layout Subsubsection*
- What other methods could I have used?
\end_layout

\begin_layout Subsubsection*
- - What is good and bad about my method?
\end_layout

\begin_layout Subsubsection*
- - What else could I have used, what are their strength and weaknesses?
\end_layout

\begin_layout Subsubsection*
- - Why did I not use those methods, but instead chose to do, what I did?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Question 5 (clustering)
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Fit in somewhere?
\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Plain Layout
When looking at the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2_4_3_scatterpoints"

\end_inset

 from above one might have noticed that the points are somewhat split into
 two or even maybe three groups.
\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Plain Layout

\emph on
k
\emph default
-mean clustering tries to classify a set of unclassified data by trying
 to find 
\emph on
k
\emph default
 means i.e.
 class center points in some data.
\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Plain Layout
When the clusters have been found one can then start to label new input
 to one of the cluster points by using 1-NN (in other/longer words; simply
 just assign the input to the nearest class)
\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Plain Layout
This can be very useful to compress a data set, ..
 although one must be aware that cluster compression is one way, once you
 compress the data you cannot derive the original input from the cluster
 points.
\end_layout

\begin_layout Plain Layout
...
 or removing noise by clustering groups of the data as single points.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now we move on to 
\emph on
k
\emph default
-mean clustering, or just clustering by short, which is good tool to extract
 even more meaning from a data set
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
, and works therefore great with PCA
\end_layout

\end_inset

.
 Clustering works by finding the 
\emph on
n
\emph default
 points which describes the data as good as possible.
 This is done by iteratively assigning each point in the input space to
 the cluster point which is the closest, then update the cluster points
 to the mean of the respectively generated cluster group, where the initial
 
\emph on
n
\emph default
 cluster points are selected at random or heuristically.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This give us the ability to group the data and ...
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
bla bla
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
- How have I done it? the code
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
Once again I have used 
\emph on
sci-kit learn
\emph default
 library, this time I have used the 
\emph on
cluster
\emph default
 module and in particualy the 
\emph on
KMeans
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
\end_layout

\end_inset

 function.
 The function simply computes the 
\emph on
n
\emph default
-mean cluster center points.
 Although it is worth noting that 
\emph on
sci-kit learn
\emph default
 using Lloyd's algorithm, which as they say it might be fast but risks falling
 into local minima.
 Although when running the code multiple times and matching the found cluster
 points against a set of averaged cluster points then the results seems
 reasonable close enough, and will not gives this much more thought as it
 will not affect our results nor discussions much, this is an importance
 issue, although not in our case.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
KMeans 
\emph default
module from 
\emph on
sci-kit learn
\emph default
 uses Lloyd's algorithm, which as they say it might be fast but risks falling
 into local minima.
 This is because it picks the initial 
\emph on
k
\emph default
 center points at random.
 I try to make up for this by making sure that the algorithm is run many
 times
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
 and let the final center points be the ones that occur the most
\end_layout

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Seems like a unfinished line.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add some algorithms....
 ?
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Results
\end_layout

\begin_layout Standard
When performing the 2-mean clustering on the galaxy data we are left with
 the following two cluster center points.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\begin_inset space \hspace{}
\length -1.6cm
\end_inset


\begin_inset Formula $[-0.166893,-0.826119,-1.210908,-1.397877,-1.512199,0.607583,-0.106192,-0.484871,-0.666617,-0.819270]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset space \hspace{}
\length -0.8cm
\end_inset


\begin_inset Formula $[1.273834,0.832120,0.168408,-0.232916,-0.403030,1.928551,1.285293,0.605494,0.213195,-0.008197]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
Although it is very hard to image where these cluster points are located
 in respect with the given data set.
 We can therefore use our PCA solution from before and project the two 10-dimens
ional center points onto the first two principal components and get the
 following two 2-dimensional center points.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
 & [-2.12378504,0.03487029]\\
 & [1.82679699,-0.02999406]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
These have been plotted in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2_5_1_scatterandcluster"

\end_inset

 along with the scatter point from figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2_4_3_scatterpoints"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Figures/2_5_1-scatterandcenter.png
	lyxscale 60
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:2_5_1_scatterandcluster"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The figure might just confirm our suspicion from before, that the galaxy
 data is somewhat reasonably separable in two groups.
 
\end_layout

\end_inset

Of course 
\emph on
k
\emph default
-mean clustering will 
\emph on
always
\emph default
 find 
\emph on
k
\emph default
 clusters even though not one of them might make sense in any useable way,
 which is in a sense one of the pitfalls of the 
\emph on
k
\emph default
-mean clustering.
 When looking at the scatter point in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2_4_3_scatterpoints"

\end_inset

 it is relatively easy to spot where the two center points would be positioned
 at, because we can see the data visualized, although for not PCA how could
 one imagine the data in higher dimensions of 3? Making it impossible to
 guess easily how many clusters a data is clustered in.
 Even with PCA we are limited to the 1-3 dimensional space, which is most
 likely not enough to visualize all clusters in input spaces that are much
 larger.
\end_layout

\begin_layout Subsection
Question 6 (kernel mean classifier)
\end_layout

\begin_layout Standard
We have some input data 
\begin_inset Formula $X$
\end_inset

 which is separated in a set of classes 
\begin_inset Formula $C$
\end_inset

 where 
\begin_inset Formula $X_{c}$
\end_inset

 denotes the subset of 
\begin_inset Formula $X$
\end_inset

 which is labeled with 
\begin_inset Formula $c$
\end_inset

 where 
\begin_inset Formula $c\in C$
\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
 One might be tricked to say that we have C clusters where mu(X_c) is the
 center point of the corresponding cluster, or the mean of that subset of
 X.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $h(x)$
\end_inset

 then loops through 
\begin_inset Formula $C$
\end_inset

 and returns the label of class 
\begin_inset Formula $c$
\end_inset

 where the distances is the smallest between 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\text{\mu}(X_{c})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h(x)=argmin_{c\in C}\left\Vert x-\mu(X_{c})\right\Vert 
\]

\end_inset


\end_layout

\begin_layout Standard
Now we want to try and redefine 
\begin_inset Formula $h(x)$
\end_inset

 to use kernel functions.
 So the goal is to rewrite 
\begin_inset Formula $h(x)$
\end_inset

 to only use the dot product operation when calculating the distance from
 
\begin_inset Formula $x$
\end_inset

 to 
\begin_inset Formula $\text{\mu}(X_{c})$
\end_inset

 and when calculating the mean of 
\begin_inset Formula $X_{c}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
We start by inserting a dummy
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
what is the correct term
\end_layout

\end_inset

 root and square
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h(x)=argmin_{c\in C}\sqrt{\left\Vert x-\mu(X_{c})\right\Vert ^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
This allows use to rewrite the square product
\begin_inset Note Note
status open

\begin_layout Plain Layout
slide 12 page 34
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
why does it that, how? what rule?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h(x)=argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -2\left\langle x,\mu(X_{c})\right\rangle +\left\langle \mu(X_{c}),\mu(X_{c})\right\rangle }
\]

\end_inset


\end_layout

\begin_layout Standard
Were 
\begin_inset Formula $\left\langle x,y\right\rangle $
\end_inset

 denotes the dot product 
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Formula $x\text{\bullet}y$
\end_inset

 need correct dot
\end_layout

\end_inset

 of 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

.
 Now we will try to redefine 
\begin_inset Formula $\mu$
\end_inset

, which we can write as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mu(X_{c})=\frac{1}{N_{c}}\sum_{x\in X_{c}}x\text{, where \ensuremath{N_{c}}denotes the number of elements in \ensuremath{X_{c}}}
\]

\end_inset


\end_layout

\begin_layout Standard
If we replace 
\begin_inset Formula $\mu$
\end_inset

 with the above notation then we get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h(x)=argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -2\left\langle x,\frac{1}{N_{c}}\sum_{x'\in X_{c}}x'\right\rangle +\left\langle \frac{1}{N_{c}}\sum_{x''\in X_{c}}x'',\frac{1}{N_{c}}\sum_{x'''\in X_{c}}x'''\right\rangle }
\]

\end_inset


\end_layout

\begin_layout Standard
Now we can use the associative and distributive law
\begin_inset Note Note
status open

\begin_layout Plain Layout
fotnote, link or some description?
\end_layout

\end_inset

 of dot products to extract those summations of out the dot products
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula 
\begin{eqnarray*}
h(x) & = & argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -2\frac{1}{N}\sum_{x'\in X_{c}}\left\langle x,x'\right\rangle +\frac{1}{N}\sum_{x''\in X_{c}}\left\langle x'',\frac{1}{N}\sum_{x'''\in X_{c}}x'''\right\rangle }\\
This?1 & = & argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -\frac{1}{N}\sum_{x'\in X_{c}}\left(2\left\langle x,x'\right\rangle +/-\left\langle x',\frac{1}{N}\sum_{x'''\in X_{c}}x'''\right\rangle \right)}\\
2 & = & argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -\frac{1}{N}\sum_{x'\in X_{c}}\left(2\left\langle x,x'\right\rangle +/-\frac{1}{N}\sum_{x'''\in X_{c}}\left\langle x',x'''\right\rangle \right)}\\
or\, this?1 & = & argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -2\frac{1}{N}\sum_{x'\in X_{c}}\left\langle x,x'\right\rangle +\frac{1}{N}\sum_{x''\in X_{c}}\frac{1}{N}\sum_{x'''\in X_{c}}\left\langle x'',x'''\right\rangle }\\
2 & = & argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -\frac{2}{N}\sum_{x'\in X_{c}}\left\langle x,x'\right\rangle +\frac{1}{N^{2}}\sum_{x''\in X_{c}}\sum_{x'''\in X_{c}}\left\langle x'',x'''\right\rangle }
\end{eqnarray*}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
h(x) & = & argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -2\frac{1}{N_{c}}\sum_{x'\in X_{c}}\left\langle x,x'\right\rangle +\frac{1}{N_{c}}\sum_{x''\in X_{c}}\left\langle x'',\frac{1}{N_{c}}\sum_{x'''\in X_{c}}x'''\right\rangle }\\
 & = & argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -2\frac{1}{N_{c}}\sum_{x'\in X_{c}}\left\langle x,x'\right\rangle +\frac{1}{N_{c}}\sum_{x''\in X_{c}}\frac{1}{N_{c}}\sum_{x'''\in X_{c}}\left\langle x'',x'''\right\rangle }\\
 & = & argmin_{c\in C}\sqrt{\left\langle x,x\right\rangle -\frac{2}{N_{c}}\sum_{x'\in X_{c}}\left\langle x,x'\right\rangle +\frac{1}{N_{c}^{2}}\sum_{x''\in X_{c}}\sum_{x'''\in X_{c}}\left\langle x'',x'''\right\rangle }
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We are given a positive definite kernel function 
\begin_inset Formula $k:X\times X\rightarrow\mathbb{R}$
\end_inset

 and we assume that there exists a Hilbert feature space 
\begin_inset Formula $\mathcal{H}$
\end_inset

 and a feature map 
\begin_inset Formula $\Phi:X\rightarrow\mathcal{H}$
\end_inset

 which allows use to define the kernel function 
\begin_inset Formula $k$
\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(x,y)=\left\langle \Phi(x),\Phi(y)\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
Which allows us to use the kernel definition to rewrite our current formula
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
If going with the first one
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\begin{eqnarray*}
h(x) & = & argmin_{c\in C}\sqrt{\left\langle \Phi(x),\Phi(x)\right\rangle -\frac{1}{N}\sum_{x'\in X_{c}}\left(2\left\langle \Phi(x),\Phi(x')\right\rangle +\frac{1}{N}\sum_{x'''\in X_{c}}\left\langle \Phi(x'),\Phi(x''')\right\rangle \right)}\\
 & = & argmin_{c\in C}\sqrt{k(x,x)-\frac{1}{N}\sum_{x'\in X_{c}}\left(2k(x,x')+\frac{1}{N}\sum_{x'''\in X_{c}}k(x',x''')\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Plain Layout
This gives us the final formula
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
h(x)=argmin_{c\in C}\sqrt{k(x,x)-\frac{1}{N}\sum_{x'\in X_{c}}\left(2k(x,x')+\frac{1}{N}\sum_{x'''\in X_{c}}k(x',x''')\right)}
\]

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
If going with the second one
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
h(x) & = & argmin_{c\in C}\sqrt{\left\langle \Phi(x),\Phi(x)\right\rangle -\frac{2}{N_{c}}\sum_{x'\in X_{c}}\left\langle \Phi(x),\Phi(x')\right\rangle +\frac{1}{N_{c}^{2}}\sum_{x''\in X_{c}}\sum_{x'''\in X_{c}}\left\langle \Phi(x''),\Phi(x''')\right\rangle }\\
 & = & argmin_{c\in C}\sqrt{k(x,x)-\frac{2}{N_{c}}\sum_{x'\in X_{c}}k(x,x')+\frac{1}{N_{c}^{2}}\sum_{x''\in X_{c}}\sum_{x'''\in X_{c}}k(x'',x''')}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This gives us the final formula
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
h(x)=argmin_{c\in C}\sqrt{k(x,x)-\frac{2}{N_{c}}\sum_{x'\in X_{c}}k(x,x')+\frac{1}{N_{c}^{2}}\sum_{x''\in X_{c}}\sum_{x'''\in X_{c}}k(x'',x''')}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Common again
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Where we have redefined the nearest mean classifier to only depend on the
 value of the kernel function 
\begin_inset Formula $k$
\end_inset

 evaluated on pairs of data points.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
One could argue that the square-root operation is redundant as it does not
 affect the relationship between the found values
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
, as it is just as multiplier a constant to the entire expression
\end_layout

\end_inset

.
 One could even argue that the first component 
\begin_inset Formula $k(x,x)$
\end_inset

 is redundant as well
\begin_inset Note Note
status open

\begin_layout Plain Layout
must argue how we can be sure of that!
\end_layout

\end_inset

 and be removing those we arrive at the final formula:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
h(x)=argmin_{c\in C}-\frac{2}{N_{c}}\sum_{x'\in X_{c}}k(x,x')+\frac{1}{N_{c}^{2}}\sum_{x''\in X_{c}}\sum_{x'''\in X_{c}}k(x'',x''')
\]

\end_inset


\end_layout

\begin_layout Standard
Note that I did notice some other ways to define the final formula although
 I like this one as, even though the last two summations might be ugly,
 they can be easily precomputed and hence somewhat minimize the computation
 time of the expression.
\end_layout

\begin_layout Section
Variable Stars
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Need intro
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Question 7 (multi-class classification)
\end_layout

\begin_layout Standard
I played around with many different options and trained several different
 models before landing on my final two; linear discriminant analysis
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/generated/sklearn.lda.LDA.html
\end_layout

\end_inset

 (LDA) and random forest
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClass
ifier.html
\end_layout

\end_inset

 for multi classification of the Variable Stars data set.
\end_layout

\begin_layout Standard
Note that the data is very scarce and some of the classes have very few
 data points assigned to them.
 This might affect the solutions greatly as the small classes might be forgotten.
\end_layout

\begin_layout Subsubsection*
Linear multi classification
\end_layout

\begin_layout Standard
The reason of choice lies in the fact that we had to implement LDA in one
 of the mandatory assignments and I found it quite fun to implement and
 work with and found LDA to be one of the ML methods that was somewhat easy
 to grasp and play with.
 The LDA build in the mandatory assignment left me an impression that LDA
 works rather well generally and as it does not have to any hyperparameters
 to worry about it is a very simple model and easy to work with
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
and also gave me some decent results in this exercise
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
The only thing that bothers me about LDA is that it assumes that all the
 class covariances are identical
\begin_inset Note Note
status open

\begin_layout Plain Layout
so what?
\end_layout

\end_inset

, as based on my current ML experience this only happens in custom designed
 data sets
\begin_inset Note Note
status open

\begin_layout Plain Layout
risky to say that?
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
When using LDA to classify the Variable Stars data set, the model is fit
 with the training data, I get an accuracy of 
\begin_inset Formula $81.5823605707\%$
\end_inset

 on the training data and an accuracy of 
\begin_inset Formula $71.3359273671\%$
\end_inset

 on the test data.
\end_layout

\begin_layout Subsubsection*
Non-linear multi classification
\end_layout

\begin_layout Standard
I find the simplicity and easy-achievable efficiency of random forests to
 be intriguing.
 Random forests makes few assumptions on the data opposed to many other
 classifiers and ...
 and it was one of the classifiers I tried out that gave the best results
 on the test set (human overfitting much?).
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
 The major downside of random forests is that they are 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
As random forests has 
\emph on
a lot
\emph default
 of hyperparameters I of course used the standard grid search and cross
 validation techniques to try and optimize these.
 Some of the main components to decide upon is the number of trees 
\begin_inset Formula $B$
\end_inset

 where i tried different numbers between 
\begin_inset Formula $1-1000$
\end_inset

 and I determine the number of max.
 features 
\begin_inset Formula $m$
\end_inset

 to be 
\begin_inset Formula $\left\lfloor \sqrt{D}\right\rfloor $
\end_inset

 where 
\begin_inset Formula $D$
\end_inset

 is the dimension of the data
\begin_inset Note Note
status open

\begin_layout Plain Layout
we know that?
\end_layout

\end_inset

.
 As I do not yet have that much experience with random forests I am a bit
 short on good values for the other parameters, but I have just tried to
 play around with them.
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
When using the build classifier I achieve an accuracy of 
\begin_inset Formula $100\%$
\end_inset

 on the training data and an accuracy of 
\begin_inset Formula $??\%$
\end_inset

 on the test data set.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Compare the two models?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Question 8 (overfitting)
\end_layout

\begin_layout Subsubsection
Traditional overfitting
\begin_inset CommandInset label
LatexCommand label
name "sub:Traditional-overfitting"

\end_inset


\end_layout

\begin_layout Standard
Considering that there are so few data points but so many features and classes
 in the Variable Star data set, one might be concerned of traditional overfittin
g as there might simply not be enough data points to create a generally
 well performing classifier.
\end_layout

\begin_layout Standard
This affects the classification in such a way that it has trouble finding
 general meaning of the data and needs to learn the training data more by
 heart to be able to get reasonable results in the training accuracy.
 Which is all we have, the training data, so it is the only set of data
 that we can use to decide wether or not our solution is good enough, and
 hence we strive to get a good result before stopping, but a good result
 (say 90% acc.) might have crossed the overfitting boundary without us noticing.
\end_layout

\begin_layout Standard
If one wants to maintain the complex classifier then one ought to get more
 training data.
\end_layout

\begin_layout Subsubsection
Parameter tweak overfitting
\end_layout

\begin_layout Standard
This has not been such a big concern when fitting my classifiers in question
 7.
 Yet, it is still a very big concern in machine learning in general, as
 once you move away from simple classifier such as KNN which only have a
 very few and conceptually simple hyperparameters, and move on to more advanced
 and complex classifiers, or regression models for that matter, you tend
 to get an increase in the number of hyperparameters, and tweaking these
 just right, without a tendency towards overfitting is very hard.
 When dealing with many hyperparameters it is often hard to distinguish
 between their meaning when comparing their values with the results of the
 build classifier and hence also hard to figure out when the chosen values
 favor the training data too much, hence overfitting.
\end_layout

\begin_layout Subsubsection
Data set selection
\end_layout

\begin_layout Standard
This will quickly result in overfitting for many of the reasons described
 above in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Traditional-overfitting"

\end_inset

.
 Seeing that we don't have many that training points and it is somewhat
 hard to get very good results with the created classifiers, one might be
 very tempted to focus on the training error and try to maximize this
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
 (if considered both the given training and test data to be one component)
\end_layout

\end_inset

 and therefore possibly overfit the classifier.
 One of the golden rules we have learned in this course is that going for
 0 empirical risk
\begin_inset Note Note
status open

\begin_layout Plain Layout
correct? Check slides
\end_layout

\end_inset

 is not the way to go.
 It is very easy to get 
\begin_inset Formula $100\%$
\end_inset

 accuracy on the training data, just use 1-nearest neighbor and you are
 (
\emph on
almost
\emph default
) guaranteed a perfect score on your training data.
 Then you have learned your training data by heart, and at the same time
 overfitted your classifier.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Write more, maybe make it more focused on something I have used in the Variable
 Stars exercise? Or just be more specific in the answer.
\end_layout

\end_inset


\end_layout

\end_body
\end_document
